[General]
version = 0.5
globstep = 90000
watsonmode = False
autoencode = False
corpus = lightweight

[Dataset]
datasettag = positive
maxlength = 10
filtervocab = 1
skiplines = False
vocabularysize = 40000

[Network]
hiddensize = 512
numlayers = 2
softmaxsamples = 0
initembeddings = True
embeddingsize = 64
embeddingsource = wiki.zh.vec

[Training (won't be restored)]
learningrate = 0.002
batchsize = 256
dropout = 0.9

